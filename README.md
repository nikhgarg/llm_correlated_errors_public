## Data
- data/final.csv: This file contains the final dataset used for all experiments. It contains the combination of scores generated by LLMs and human labelling. **Details documented in notebooks/5_Data.ipynb**.
- data/upwork-jobs.csv (can be accessed [here](https://www.kaggle.com/datasets/asaniczka/upwork-job-postings-dataset-2024-50k-records)): This file contains the raw job description dataset used.
- data/resume_samples.txt (can be accessed [here](https://github.com/florex/resume_corpus)), data/Resume.csv (can be accessed [here](https://www.kaggle.com/datasets/snehaanbhawal/resume-dataset)): These files contain the raw resume dataset used.

### HELM Data
- all_mmlu_data_limitedcols.csv: This file contains a summary of all the data in prediction_data. **Generated by scripts/download_helm_data.py**.
- model_accuracy.csv: Contains the accuracy of each HELM model on the MMLU data.
- model_correlations.csv: This is the base model correlations and other metadata between two model's scores. **Generated by helm_data_generation.py**.
  1. chi_square: The raw chi square value by comparing the distribution of the two model's scores.
  2. p_value: The p-value of the associated chi_square score.
  3. overall_agreement: The proportion of all questions to questions in which the two models agree with each other.
  4. both_correct_Rate: Proportion of all questions to questions where both models are correct.
  5. agreement_rate_when_either_wrong: The proportion of questions in which two models agree and get it wrong to questions where either model gets a question wrong.
  6. agreement_rate_when_both_wrong: The proportion of all questions to questions where the 2 models give the wrong answer and agree.
  7. contingency_table: Contingency table of scores.
  8. model1: The first model being evaluated.
  9. model2: The second model being evaluated.
- model_overview.csv: Contains metadata about each HELM model.

### Hugging Face
- relevant_models.txt: A predefined list of relevant models. **Detailed in Section <TODO>**.
- pandas_urls_2100.csv: A mapping of model name to the hugging face data location.
- model_to_location.csv: The location of a model's data within the processed_data directory.
- hugging_face.csv: All metadata related to models taken from [Hugging Face Leaderboard](https://open-llm-leaderboard-open-llm-leaderboard.hf.space/api/leaderboard/formatted).
- hf_model_accuracy: The accuracy of each model on the MMLU dataset.
- hf_model_correlation.csv: Similar to the HELM model correlation but with Hugging Face models.
- hf_featured_models.csv: The model correlation data but with relevant features and model accuracy to create a regression dataset.
    **Features**
    - params_billions: The number of parameters measured in billions.
    - is_moe: Whether the model is a mixture of experts model or not.
    - architecture: The base model architecture for the LLM model.

prediction_data contains all the scored MMLU files from the [HELM online data store](https://crfm.stanford.edu/helm/lite/latest/#/runs) and we compile this into `all_mmlu_data_limitedcols.csv` which is a tall form of this data across all relevant HELM models.

processed_data contains all the scored MMLU files from Hugging Face per model and we reference this through the mapping `model_to_location.csv`.

regressions contains .tex files showing regression on multiple y variables:
1. agreement_rate_when_both_wrong
2. agreement_rate_when_either_wrong 
3. both_correct_rate
4. overall_agreement_rate
against a myriad of model features. This is cited in the paper in section 3.2.

## Scripts/Utilities
- scripts/constants.py: Contains information of prompts used for LLMs.
- scripts/deferred_acceptance.py: Contains helper functions that execute stable matching given a set of applicants and firms.
- scripts/experiments.py: Contains helper functions that execute matching markets experiment. **Details documented in notebooks/5.1_Systemic_Exclusion.ipynb and notebooks/5.2_Matching_Markets.ipynb**.
- scripts/helper_scripts.py: Contains helper functions to plot various correlation heatmaps. **Details documented in notebooks/Correlation_regression.ipynb**.
- scripts/model_info.py: Contains a dictionary of LLM configurations that are used on AWS Bedrock.
- scripts/setup_data.py: Contains helper functions that processes raw data files and extract various insights. **Details documented in notebooks/5_Data.ipynb**.
- scripts/utilities.py: Contains utility functions for parsing model outputs, hashing, and calling various LLM APIs on AWS Bedrock.
- helm_data_generation.py: Generates the model correlation and accuracy datasets.
- helm_regression.py: Performs regression on the HELM dataset using the data generated in `helm_data_generation.py`.
- hugging_face_data_generation.py: Generates the model correlation and accuracy datasets for Hugging Face models.
- hugging_face_regression.py: Performs regression on the Hugging Face datasets using the data generated in `hugging_face_data_generation.py`.
- plot_helm.py: Generates the figures seen in the report.
- plot_hugging_face.py: Generates the figures seen in the report.

## Final Figures
/final_figures directory contains the files of the final figures and tables related to the "Markets" section of the paper and its corresponding appendix.

## To Replicate Generating Figures on Paper

First, create and activate a new Conda environment:
```bash
conda create -n llm_env python=3.10
conda activate llm_env
```

Second, install required packages:
```bash
pip install -r requirements.txt
```

To replicate the entire data collection process, users need access to the following APIs: AWS Bedrock, OpenAI, and Anthropic.

If users do have access, then they can start with data processing by running code on **notebooks/5_Data.ipynb**. Users may need to modify file paths accordingly. If not, users can scroll all the way down and load the finalized version that we used for experiments in **data/final.csv**.

Users can then proceed (assuming they are using the existing data/final.csv or have generated their own version of data/final.csv) to run all the cells on notebooks/Correlation_regression.ipynb to generate all figures related to model correlation in section 5 of the paper, notebooks/5.1_Systemic_Exclusion.ipynb to generate all figures related to section 5.1 of the paper, and notebooks/5.2_Matching_Markets.ipynb to generate all figures related to section 5.2 of the paper.

To reproduce the findings and plots for the Correlated Errors section, users can run the data generation, regression, and plot scripts, in that order, for both HELM and Hugging Face, providing the processed_data and prediction_data, as well as ancillary files, are present.

### Data Download
In order to properly retrieve the files that are necessary for Hugging Face and HELM analysis, we recommend the following:
- HELM Data: This data should live in prediction_data and is downloaded from the HELM website. The files span across multiple different scenarios (e.g., astronomy, high_school_mathematics) and you should specify which models are needed for this. We provide a way to download the data (./scripts/download_helm_data.py) that we used, as well as the files all_mmlu_data_limitedcols.csv and model_overview.csv which are necessary for all the further data generation, regression, and plotting.
- Hugging Face Data: This data should live in processed_data and is retrieved from the Hugging Face website. The files are JSON and retrieved per MMLU dataset per model. You must go to the hugging face website ("https://open-llm-leaderboard-open-llm-leaderboard.hf.space/?__theme=light#/") and for each model of interest, you have to
1. Navigate to the details tab (direct link per model included in model_to_file.csv)
2. Accept the repository terms and usage 
3. Download the relevant file (listed per model in model_to_file.csv)
4. Convert that file into a CSV through pandas
Unfortunately due to the scale of models, it is unfeasible to have a single tall dataframe similar to what the HELM data does. Additionally, you should create a CSV called model_to_location.csv that maps a model's name to the file's location in processed_data. 
